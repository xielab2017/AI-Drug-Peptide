#!/usr/bin/env python3
"""
Prefectå·¥ä½œæµè„šæœ¬ - è‚½æ®µè¯ç‰©å¼€å‘å®Œæ•´æµç¨‹
==========================================

åŠŸèƒ½ï¼š
1. å®šä¹‰5ä¸ªä»»åŠ¡ï¼šå‚æ•°åˆå§‹åŒ–â†’æ•°æ®æ‹‰å–â†’åˆ†æ³Œè·¯å¾„+å—ä½“åˆ†æâ†’è‚½æ®µä¼˜åŒ–â†’å¯è§†åŒ–+æŠ¥å‘Šç”Ÿæˆ
2. ä»»åŠ¡ä¾èµ–ï¼šå‚æ•°åˆå§‹åŒ–å®Œæˆâ†’æ•°æ®æ‹‰å–â†’åˆ†æâ†’ä¼˜åŒ–â†’æŠ¥å‘Š
3. é…ç½®å‘Šè­¦ï¼šä»»æ„ä»»åŠ¡å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨å‘é€é‚®ä»¶è‡³æŒ‡å®šé‚®ç®±ï¼ˆé…ç½®åœ¨config.jsonï¼‰
4. æ”¯æŒ"æ–­ç‚¹ç»­è·‘"ï¼šæŸä¸€æ­¥å¤±è´¥ä¿®å¤åï¼Œä»å¤±è´¥æ­¥éª¤é‡æ–°æ‰§è¡Œ
5. æ‰§è¡Œå®Œæˆåï¼Œè¾“å‡º"æµç¨‹æ€»ç»“æŠ¥å‘Š"ï¼ˆTXTï¼‰ï¼Œæ˜¾ç¤ºå„æ­¥éª¤è€—æ—¶ã€æˆåŠŸ/å¤±è´¥çŠ¶æ€

ä½œè€…ï¼šAI-Drug Peptide Project
æ—¥æœŸï¼š2024
"""

import os
import sys
import json
import time
import logging
import smtplib
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders

# Prefect imports
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.blocks.system import Secret
from prefect.artifacts import create_markdown_artifact
from prefect.filesystems import LocalFileSystem

# é¡¹ç›®æ¨¡å—å¯¼å…¥
try:
    from input_init import ProteinInputInitializer
    from data_fetch_robust import DataFetcher
    from secretion_analysis import SecretionAnalyzer
    from peptide_optim import PeptideOptimizationPipeline
    from report_generator import ReportGenerator
except ImportError as e:
    print(f"Warning: Some modules not available: {e}")
    # å®šä¹‰å ä½ç¬¦ç±»
    class ProteinInputInitializer:
        def run(self): return None
    class DataFetcher:
        def run_all(self): return None
    class SecretionAnalyzer:
        def run_analysis(self): return None
    class PeptideOptimizationPipeline:
        def optimize_peptides(self): return None
    class ReportGenerator:
        def generate_report(self): return None

# é…ç½®å¸¸é‡
CONFIG_FILE = "config/config.json"
WORKFLOW_STATE_FILE = "workflow_state.json"
SUMMARY_REPORT_FILE = "workflow_summary_report.txt"

@dataclass
class TaskResult:
    """ä»»åŠ¡æ‰§è¡Œç»“æœ"""
    task_name: str
    success: bool
    start_time: datetime
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    error_message: Optional[str] = None
    output_data: Optional[Dict[str, Any]] = None
    checkpoint_data: Optional[Dict[str, Any]] = None

@dataclass
class WorkflowState:
    """å·¥ä½œæµçŠ¶æ€ç®¡ç†"""
    current_step: int = 1
    completed_steps: List[int] = None
    failed_steps: List[int] = None
    task_results: List[TaskResult] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    def __post_init__(self):
        if self.completed_steps is None:
            self.completed_steps = []
        if self.failed_steps is None:
            self.failed_steps = []
        if self.task_results is None:
            self.task_results = []

class WorkflowManager:
    """å·¥ä½œæµç®¡ç†å™¨ - è´Ÿè´£çŠ¶æ€ç®¡ç†å’Œæ–­ç‚¹ç»­è·‘"""
    
    def __init__(self, state_file: str = WORKFLOW_STATE_FILE):
        self.state_file = Path(state_file)
        self.state = self.load_state()
    
    def load_state(self) -> WorkflowState:
        """åŠ è½½å·¥ä½œæµçŠ¶æ€"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è½¬æ¢datetimeå­—ç¬¦ä¸²
                if data.get('start_time'):
                    data['start_time'] = datetime.fromisoformat(data['start_time'])
                if data.get('end_time'):
                    data['end_time'] = datetime.fromisoformat(data['end_time'])
                
                # è½¬æ¢TaskResultå¯¹è±¡
                if data.get('task_results'):
                    task_results = []
                    for tr_data in data['task_results']:
                        if tr_data.get('start_time'):
                            tr_data['start_time'] = datetime.fromisoformat(tr_data['start_time'])
                        if tr_data.get('end_time'):
                            tr_data['end_time'] = datetime.fromisoformat(tr_data['end_time'])
                        task_results.append(TaskResult(**tr_data))
                    data['task_results'] = task_results
                
                return WorkflowState(**data)
            except Exception as e:
                logging.warning(f"Failed to load state file: {e}")
        
        return WorkflowState()
    
    def save_state(self):
        """ä¿å­˜å·¥ä½œæµçŠ¶æ€"""
        try:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            state_data = {
                'current_step': self.state.current_step,
                'completed_steps': self.state.completed_steps,
                'failed_steps': self.state.failed_steps,
                'task_results': [],
                'start_time': self.state.start_time.isoformat() if self.state.start_time else None,
                'end_time': self.state.end_time.isoformat() if self.state.end_time else None
            }
            
            # è½¬æ¢TaskResultå¯¹è±¡
            for tr in self.state.task_results:
                tr_data = {
                    'task_name': tr.task_name,
                    'success': tr.success,
                    'start_time': tr.start_time.isoformat(),
                    'end_time': tr.end_time.isoformat() if tr.end_time else None,
                    'duration': tr.duration,
                    'error_message': tr.error_message,
                    'output_data': tr.output_data,
                    'checkpoint_data': tr.checkpoint_data
                }
                state_data['task_results'].append(tr_data)
            
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state_data, f, indent=2, ensure_ascii=False)
            
            logging.info(f"Workflow state saved to {self.state_file}")
        except Exception as e:
            logging.error(f"Failed to save state: {e}")
    
    def can_resume_from_step(self, step: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»æŒ‡å®šæ­¥éª¤æ¢å¤"""
        return step in self.state.completed_steps
    
    def mark_step_completed(self, step: int, result: TaskResult):
        """æ ‡è®°æ­¥éª¤å®Œæˆ"""
        if step not in self.state.completed_steps:
            self.state.completed_steps.append(step)
        
        # ç§»é™¤å¤±è´¥è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if step in self.state.failed_steps:
            self.state.failed_steps.remove(step)
        
        # æ›´æ–°ä»»åŠ¡ç»“æœ
        self.state.task_results.append(result)
        self.save_state()
    
    def mark_step_failed(self, step: int, error_message: str):
        """æ ‡è®°æ­¥éª¤å¤±è´¥"""
        if step not in self.state.failed_steps:
            self.state.failed_steps.append(step)
        
        # è®°å½•å¤±è´¥ç»“æœ
        failed_result = TaskResult(
            task_name=f"Step_{step}",
            success=False,
            start_time=datetime.now(),
            end_time=datetime.now(),
            duration=0.0,
            error_message=error_message
        )
        self.state.task_results.append(failed_result)
        self.save_state()

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - è´Ÿè´£æ¸…ç†å’Œåˆ†æç›¸å…³çš„ç¼“å­˜"""
    
    def __init__(self, cache_base_dir: str = "./data/cache"):
        self.cache_base_dir = Path(cache_base_dir)
        self.cache_base_dir.mkdir(parents=True, exist_ok=True)
    
    def clear_all_cache(self) -> bool:
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
        try:
            if self.cache_base_dir.exists():
                import shutil
                shutil.rmtree(self.cache_base_dir)
                self.cache_base_dir.mkdir(parents=True, exist_ok=True)
                logging.info(f"Cleared all cache in {self.cache_base_dir}")
                return True
        except Exception as e:
            logging.error(f"Failed to clear cache: {e}")
            return False
    
    def clear_protein_specific_cache(self, protein_id: str) -> bool:
        """æ¸…ç†ç‰¹å®šè›‹ç™½è´¨çš„ç¼“å­˜"""
        try:
            # æ¸…ç†ä¸ç‰¹å®šè›‹ç™½è´¨ç›¸å…³çš„ç¼“å­˜æ–‡ä»¶
            cache_patterns = [
                f"*{protein_id}*",
                "sequence_cache.csv",
                "geo_cache.csv", 
                "hsd_cache.csv",
                "pdb_cache/*.pdb",
                "docking_logs/*.log",
                "conservation_results.csv",
                "binding_energy_chart.png",
                "conservation_heatmap.png"
            ]
            
            cleared_files = 0
            for pattern in cache_patterns:
                for file_path in self.cache_base_dir.glob(pattern):
                    if file_path.is_file():
                        file_path.unlink()
                        cleared_files += 1
                    elif file_path.is_dir():
                        import shutil
                        shutil.rmtree(file_path)
                        cleared_files += 1
            
            logging.info(f"Cleared {cleared_files} cache files for protein {protein_id}")
            return True
            
        except Exception as e:
            logging.error(f"Failed to clear protein-specific cache: {e}")
            return False
    
    def clear_workflow_state(self) -> bool:
        """æ¸…ç†å·¥ä½œæµçŠ¶æ€æ–‡ä»¶"""
        try:
            state_files = [
                "workflow_state.json",
                "workflow_summary_report.txt",
                "workflow.log"
            ]
            
            for state_file in state_files:
                file_path = Path(state_file)
                if file_path.exists():
                    file_path.unlink()
                    logging.info(f"Cleared state file: {state_file}")
            
            return True
            
        except Exception as e:
            logging.error(f"Failed to clear workflow state: {e}")
            return False

class EmailNotifier:
    """é‚®ä»¶é€šçŸ¥ç³»ç»Ÿ"""
    
    def __init__(self, config_file: str = CONFIG_FILE):
        self.config = self.load_config(config_file)
        self.smtp_config = self.config.get('email_notification', {})
    
    def load_config(self, config_file: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Failed to load config file {config_file}: {e}")
            return {}
    
    def send_notification(self, subject: str, message: str, 
                         attachment_path: Optional[str] = None,
                         is_error: bool = False) -> bool:
        """å‘é€é‚®ä»¶é€šçŸ¥"""
        if not self.smtp_config.get('enabled', False):
            logging.info("Email notifications disabled")
            return True
        
        try:
            # é‚®ä»¶é…ç½®
            smtp_server = self.smtp_config.get('smtp_server', 'smtp.gmail.com')
            smtp_port = self.smtp_config.get('smtp_port', 587)
            sender_email = self.smtp_config.get('sender_email')
            sender_password = self.smtp_config.get('sender_password')
            recipient_email = self.smtp_config.get('recipient_email')
            
            if not all([sender_email, sender_password, recipient_email]):
                logging.warning("Email configuration incomplete")
                return False
            
            # åˆ›å»ºé‚®ä»¶
            msg = MIMEMultipart()
            msg['From'] = sender_email
            msg['To'] = recipient_email
            msg['Subject'] = subject
            
            # é‚®ä»¶æ­£æ–‡
            body = f"""
å·¥ä½œæµé€šçŸ¥ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{message}

---
æ­¤é‚®ä»¶ç”±Prefectå·¥ä½œæµç³»ç»Ÿè‡ªåŠ¨å‘é€
            """
            
            msg.attach(MIMEText(body, 'plain', 'utf-8'))
            
            # æ·»åŠ é™„ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
            if attachment_path and Path(attachment_path).exists():
                with open(attachment_path, "rb") as attachment:
                    part = MIMEBase('application', 'octet-stream')
                    part.set_payload(attachment.read())
                    encoders.encode_base64(part)
                    part.add_header(
                        'Content-Disposition',
                        f'attachment; filename= {Path(attachment_path).name}'
                    )
                    msg.attach(part)
            
            # å‘é€é‚®ä»¶
            server = smtplib.SMTP(smtp_server, smtp_port, timeout=10)
            server.starttls()
            server.login(sender_email, sender_password)
            text = msg.as_string()
            server.sendmail(sender_email, recipient_email, text)
            server.quit()
            
            logging.info(f"Email notification sent: {subject}")
            return True
            
        except Exception as e:
            logging.error(f"Failed to send email notification: {e}")
            return False

# Prefectä»»åŠ¡å®šä¹‰
@task(name="ç¼“å­˜æ¸…ç†", retries=1, retry_delay_seconds=10)
def task_cache_clearing(protein_id: str = None) -> TaskResult:
    """ä»»åŠ¡0ï¼šç¼“å­˜æ¸…ç†ï¼ˆå¯é€‰ï¼‰"""
    logger = get_run_logger()
    start_time = datetime.now()
    
    try:
        logger.info("å¼€å§‹ç¼“å­˜æ¸…ç†ä»»åŠ¡...")
        
        cache_manager = CacheManager()
        
        if protein_id:
            # æ¸…ç†ç‰¹å®šè›‹ç™½è´¨çš„ç¼“å­˜
            success = cache_manager.clear_protein_specific_cache(protein_id)
            logger.info(f"æ¸…ç†è›‹ç™½è´¨ {protein_id} çš„ç¼“å­˜: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            success = cache_manager.clear_all_cache()
            logger.info(f"æ¸…ç†æ‰€æœ‰ç¼“å­˜: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        # æ¸…ç†å·¥ä½œæµçŠ¶æ€
        state_cleared = cache_manager.clear_workflow_state()
        logger.info(f"æ¸…ç†å·¥ä½œæµçŠ¶æ€: {'æˆåŠŸ' if state_cleared else 'å¤±è´¥'}")
        
        logger.info("ç¼“å­˜æ¸…ç†å®Œæˆ")
        return TaskResult(
            task_name="ç¼“å­˜æ¸…ç†",
            success=True,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            output_data={"cache_cleared": success, "state_cleared": state_cleared},
            checkpoint_data={"cache_cleared": success}
        )
            
    except Exception as e:
        logger.error(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
        return TaskResult(
            task_name="ç¼“å­˜æ¸…ç†",
            success=False,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            error_message=str(e)
        )

@task(name="å‚æ•°åˆå§‹åŒ–", retries=2, retry_delay_seconds=30)
def task_parameter_initialization() -> TaskResult:
    """ä»»åŠ¡1ï¼šå‚æ•°åˆå§‹åŒ–"""
    logger = get_run_logger()
    start_time = datetime.now()
    
    try:
        logger.info("å¼€å§‹å‚æ•°åˆå§‹åŒ–ä»»åŠ¡...")
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        config_file = Path(CONFIG_FILE)
        if not config_file.exists():
            logger.warning(f"é…ç½®æ–‡ä»¶ {CONFIG_FILE} ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®")
            # åˆ›å»ºé»˜è®¤é…ç½®
            default_config = {
                "target_protein": {
                    "name": "THBS4",
                    "structure_path": "./structures/thbs4.pdb"
                },
                "conservation_analysis": {
                    "target_species": {
                        "human": {"taxonomy_id": 9606, "scientific_name": "Homo sapiens"},
                        "mouse": {"taxonomy_id": 10090, "scientific_name": "Mus musculus"}
                    },
                    "conservation_threshold": 0.8
                },
                "docking": {
                    "box_size": [20, 20, 20],
                    "energy_threshold": -7.0,
                    "max_runs": 3
                },
                "paths": {
                    "cache_dir": "./cache",
                    "dump_dir": "./cache/docking_logs",
                    "receptor_cache_dir": "./cache/receptors"
                }
            }
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        logger.info("å‚æ•°åˆå§‹åŒ–å®Œæˆ")
        return TaskResult(
            task_name="å‚æ•°åˆå§‹åŒ–",
            success=True,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            output_data={"config_file": str(config_file), "config_data": config_data},
            checkpoint_data={"config_file": str(config_file)}
        )
            
    except Exception as e:
        logger.error(f"å‚æ•°åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return TaskResult(
            task_name="å‚æ•°åˆå§‹åŒ–",
            success=False,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            error_message=str(e)
        )

@task(name="æ•°æ®æ‹‰å–", retries=3, retry_delay_seconds=60)
def task_data_fetching(init_result: TaskResult, protein_id: str = None) -> TaskResult:
    """ä»»åŠ¡2ï¼šæ•°æ®æ‹‰å–"""
    logger = get_run_logger()
    start_time = datetime.now()
    
    try:
        logger.info("å¼€å§‹æ•°æ®æ‹‰å–ä»»åŠ¡...")
        
        # ä½¿ç”¨å¥å£®çš„æ•°æ®è·å–å™¨
        fetcher = DataFetcher(protein_id=protein_id, force_refresh=True)
        fetcher.run_all()
        
        logger.info("æ•°æ®æ‹‰å–å®Œæˆ")
        return TaskResult(
            task_name="æ•°æ®æ‹‰å–",
            success=True,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            checkpoint_data={"cache_dir": "./cache"}
        )
        
    except Exception as e:
        logger.error(f"æ•°æ®æ‹‰å–å¤±è´¥: {str(e)}")
        return TaskResult(
            task_name="æ•°æ®æ‹‰å–",
            success=False,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            error_message=str(e)
        )

@task(name="åˆ†æ³Œè·¯å¾„åˆ†æ", retries=2, retry_delay_seconds=45)
def task_secretion_analysis(fetch_result: TaskResult, protein_id: str = None) -> TaskResult:
    """ä»»åŠ¡3ï¼šåˆ†æ³Œè·¯å¾„åˆ†æ"""
    logger = get_run_logger()
    start_time = datetime.now()
    
    try:
        logger.info("å¼€å§‹åˆ†æ³Œè·¯å¾„åˆ†æä»»åŠ¡...")
        
        # æ‰§è¡Œåˆ†æ³Œè·¯å¾„åˆ†æ
        # ä»æ•°æ®åº“è·å–è›‹ç™½è´¨åç§°ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„protein_id
        protein_name = "UnknownProtein"  # é»˜è®¤å€¼
        try:
            from sqlalchemy import create_engine, text
            engine = create_engine('postgresql://postgres:password@localhost:5432/peptide_research')
            with engine.connect() as conn:
                if protein_id:
                    # ä½¿ç”¨ä¼ å…¥çš„protein_idæŸ¥è¯¢
                    result = conn.execute(text('SELECT protein_name FROM target_proteins WHERE protein_id = :protein_id'), 
                                        {'protein_id': protein_id})
                else:
                    # ä½¿ç”¨é»˜è®¤æŸ¥è¯¢
                    result = conn.execute(text('SELECT protein_name FROM target_proteins LIMIT 1'))
                row = result.fetchone()
                if row and row[0]:
                    protein_name = row[0]
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨protein_idä½œä¸ºåç§°
                    protein_name = protein_id or "UnknownProtein"
        except Exception as e:
            logger.warning(f"Failed to get protein name from database: {e}")
            protein_name = protein_id or "UnknownProtein"
        
        # åˆ›å»ºè›‹ç™½ç‰¹å®šçš„è¾“å‡ºç›®å½•
        protein_safe_name = "".join(c for c in protein_name if c.isalnum() or c == '_')
        output_base_dir = Path("output") / protein_safe_name
        output_base_dir.mkdir(parents=True, exist_ok=True)
        
        analyzer = SecretionAnalyzer(protein_name=protein_name)
        analysis_result = analyzer.run_full_analysis()
        
        logger.info("åˆ†æ³Œè·¯å¾„åˆ†æå®Œæˆ")
        return TaskResult(
            task_name="åˆ†æ³Œè·¯å¾„åˆ†æ",
            success=True,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            output_data=analysis_result,
            checkpoint_data={"secretion_analysis": "completed"}
        )
        
    except Exception as e:
        logger.error(f"åˆ†æ³Œè·¯å¾„åˆ†æå¤±è´¥: {str(e)}")
        return TaskResult(
            task_name="åˆ†æ³Œè·¯å¾„åˆ†æ",
            success=False,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            error_message=str(e)
        )

@task(name="STRINGç›¸äº’ä½œç”¨åˆ†æ", retries=2, retry_delay_seconds=60)
def task_string_interaction_analysis(secretion_result: TaskResult, protein_id: str = None) -> TaskResult:
    """ä»»åŠ¡4ï¼šSTRINGç›¸äº’ä½œç”¨åˆ†æ"""
    logger = get_run_logger()
    start_time = datetime.now()
    
    try:
        logger.info("å¼€å§‹STRINGç›¸äº’ä½œç”¨åˆ†æä»»åŠ¡...")
        
        # å¯¼å…¥STRINGåˆ†ææ¨¡å—
        from step1_string_interaction import STRINGInteractionAnalysis
        
        # åˆ›å»ºé…ç½®
        config = {
            'target_protein_id': protein_id,
            'species_id': 9606,  # Homo sapiens
            'confidence_threshold': 0.9,
            'output_dir': 'cache'
        }
        
        # åˆå§‹åŒ–åˆ†æå™¨
        analysis = STRINGInteractionAnalysis(config=config)
        
        # è¿è¡Œåˆ†æ
        receptor_candidates = analysis.analyze_interactions(confidence_threshold=0.9)
        
        if not receptor_candidates.empty:
            # ä¿å­˜ç»“æœ
            output_file = analysis.save_results(receptor_candidates, "string_receptors.csv")
            logger.info(f"STRINGåˆ†æå®Œæˆï¼Œè¯†åˆ«å‡º {len(receptor_candidates)} ä¸ªæ½œåœ¨å—ä½“")
            logger.info(f"ç»“æœä¿å­˜åˆ°: {output_file}")
            
            analysis_result = {
                "receptor_count": len(receptor_candidates),
                "output_file": output_file,
                "top_candidates": receptor_candidates.head(10).to_dict('records')
            }
        else:
            logger.warning("æœªè¯†åˆ«å‡ºæ½œåœ¨å—ä½“")
            analysis_result = {
                "receptor_count": 0,
                "output_file": None,
                "top_candidates": []
            }
        
        return TaskResult(
            task_name="STRINGç›¸äº’ä½œç”¨åˆ†æ",
            success=True,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            output_data=analysis_result,
            checkpoint_data={"string_analysis": "completed"}
        )
        
    except Exception as e:
        logger.error(f"STRINGç›¸äº’ä½œç”¨åˆ†æå¤±è´¥: {str(e)}")
        return TaskResult(
            task_name="STRINGç›¸äº’ä½œç”¨åˆ†æ",
            success=False,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            error_message=str(e)
        )

@task(name="åˆ†å­å¯¹æ¥é¢„æµ‹", retries=2, retry_delay_seconds=90)
def task_docking_prediction(string_result: TaskResult, protein_id: str = None) -> TaskResult:
    """ä»»åŠ¡5ï¼šåˆ†å­å¯¹æ¥é¢„æµ‹"""
    logger = get_run_logger()
    start_time = datetime.now()
    
    try:
        logger.info("å¼€å§‹åˆ†å­å¯¹æ¥é¢„æµ‹ä»»åŠ¡...")
        
        # å¯¼å…¥å¯¹æ¥é¢„æµ‹æ¨¡å—
        from step2_docking_prediction import AutoDockDockingPredictor
        
        # åˆå§‹åŒ–é¢„æµ‹å™¨
        predictor = AutoDockDockingPredictor()
        
        # è¿è¡Œé¢„æµ‹
        predictor.run_prediction("cache/string_receptors.csv")
        
        logger.info("åˆ†å­å¯¹æ¥é¢„æµ‹å®Œæˆ")
        
        analysis_result = {
            "docking_completed": True,
            "receptor_file": "cache/string_receptors.csv",
            "results_file": "cache/docking_results.csv"
        }
        
        return TaskResult(
            task_name="åˆ†å­å¯¹æ¥é¢„æµ‹",
            success=True,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            output_data=analysis_result,
            checkpoint_data={"docking_prediction": "completed"}
        )
        
    except Exception as e:
        logger.error(f"åˆ†å­å¯¹æ¥é¢„æµ‹å¤±è´¥: {str(e)}")
        return TaskResult(
            task_name="åˆ†å­å¯¹æ¥é¢„æµ‹",
            success=False,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            error_message=str(e)
        )

@task(name="è‚½æ®µä¼˜åŒ–", retries=2, retry_delay_seconds=60)
def task_peptide_optimization(analysis_result: TaskResult) -> TaskResult:
    """ä»»åŠ¡4ï¼šè‚½æ®µä¼˜åŒ–"""
    logger = get_run_logger()
    start_time = datetime.now()
    
    try:
        logger.info("å¼€å§‹è‚½æ®µä¼˜åŒ–ä»»åŠ¡...")
        
        # æ‰§è¡Œè‚½æ®µä¼˜åŒ–
        optimizer = PeptideOptimizationPipeline()
        optimization_result = optimizer.optimize_peptides()
        
        logger.info("è‚½æ®µä¼˜åŒ–å®Œæˆ")
        return TaskResult(
            task_name="è‚½æ®µä¼˜åŒ–",
            success=True,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            output_data=optimization_result,
            checkpoint_data={"optimized_peptides": "completed"}
        )
        
    except Exception as e:
        logger.error(f"è‚½æ®µä¼˜åŒ–å¤±è´¥: {str(e)}")
        return TaskResult(
            task_name="è‚½æ®µä¼˜åŒ–",
            success=False,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            error_message=str(e)
        )

@task(name="å¯è§†åŒ–+æŠ¥å‘Šç”Ÿæˆ", retries=1, retry_delay_seconds=30)
def task_visualization_reporting(optimization_result: TaskResult, protein_id: str = None) -> TaskResult:
    """ä»»åŠ¡5ï¼šå¯è§†åŒ–+æŠ¥å‘Šç”Ÿæˆ"""
    logger = get_run_logger()
    start_time = datetime.now()
    
    try:
        logger.info("å¼€å§‹å¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡...")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        reporter = ReportGenerator(config_path="config/config.yaml")
        
        # ä»æ•°æ®åº“è·å–å®é™…çš„è›‹ç™½è´¨ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„protein_id
        protein_name = "UnknownProtein"  # é»˜è®¤å€¼
        organism = "Unknown"  # é»˜è®¤å€¼
        
        try:
            from sqlalchemy import create_engine, text
            engine = create_engine('postgresql://postgres:password@localhost:5432/peptide_research')
            with engine.connect() as conn:
                if protein_id:
                    # ä½¿ç”¨ä¼ å…¥çš„protein_idæŸ¥è¯¢
                    result = conn.execute(text('SELECT protein_name, organism FROM target_proteins WHERE protein_id = :protein_id'), 
                                        {'protein_id': protein_id})
                else:
                    # ä½¿ç”¨é»˜è®¤æŸ¥è¯¢
                    result = conn.execute(text('SELECT protein_name, organism FROM target_proteins LIMIT 1'))
                row = result.fetchone()
                if row:
                    protein_name = row[0] or (protein_id or "UnknownProtein")
                    organism = row[1] or "Unknown"
                else:
                    protein_name = protein_id or "UnknownProtein"
        except Exception as e:
            logger.warning(f"Failed to get protein info from database: {e}")
            protein_name = protein_id or "UnknownProtein"
        
        report_result = reporter.generate_report(protein_name=protein_name, species_list=[organism])
        
        logger.info("å¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return TaskResult(
            task_name="å¯è§†åŒ–+æŠ¥å‘Šç”Ÿæˆ",
            success=True,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            output_data=report_result,
            checkpoint_data={"reports_generated": "completed"}
        )
        
    except Exception as e:
        logger.error(f"å¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        return TaskResult(
            task_name="å¯è§†åŒ–+æŠ¥å‘Šç”Ÿæˆ",
            success=False,
            start_time=start_time,
            end_time=datetime.now(),
            duration=(datetime.now() - start_time).total_seconds(),
            error_message=str(e)
        )

@task(name="ç”Ÿæˆæµç¨‹æ€»ç»“æŠ¥å‘Š")
def task_generate_summary_report(all_results: List[TaskResult]) -> str:
    """ç”Ÿæˆæµç¨‹æ€»ç»“æŠ¥å‘Š"""
    logger = get_run_logger()
    
    try:
        logger.info("ç”Ÿæˆæµç¨‹æ€»ç»“æŠ¥å‘Š...")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tasks = len(all_results)
        successful_tasks = sum(1 for r in all_results if r.success)
        failed_tasks = total_tasks - successful_tasks
        
        total_duration = sum(r.duration or 0 for r in all_results)
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = f"""
è‚½æ®µè¯ç‰©å¼€å‘å·¥ä½œæµæ‰§è¡Œæ€»ç»“æŠ¥å‘Š
=====================================

æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
å·¥ä½œæµID: {os.getenv('PREFECT_FLOW_RUN_ID', 'N/A')}

æ€»ä½“ç»Ÿè®¡
--------
æ€»ä»»åŠ¡æ•°: {total_tasks}
æˆåŠŸä»»åŠ¡æ•°: {successful_tasks}
å¤±è´¥ä»»åŠ¡æ•°: {failed_tasks}
æˆåŠŸç‡: {(successful_tasks/total_tasks*100):.1f}%
æ€»è€—æ—¶: {total_duration:.2f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)

è¯¦ç»†ä»»åŠ¡æ‰§è¡Œæƒ…å†µ
----------------
"""
        
        for i, result in enumerate(all_results, 1):
            status = "âœ… æˆåŠŸ" if result.success else "âŒ å¤±è´¥"
            duration_str = f"{result.duration:.2f}ç§’" if result.duration else "N/A"
            
            report_content += f"""
ä»»åŠ¡ {i}: {result.task_name}
çŠ¶æ€: {status}
å¼€å§‹æ—¶é—´: {result.start_time.strftime('%H:%M:%S')}
ç»“æŸæ—¶é—´: {result.end_time.strftime('%H:%M:%S') if result.end_time else 'N/A'}
è€—æ—¶: {duration_str}
"""
            
            if result.error_message:
                report_content += f"é”™è¯¯ä¿¡æ¯: {result.error_message}\n"
            
            if result.checkpoint_data:
                report_content += f"æ£€æŸ¥ç‚¹æ•°æ®: {json.dumps(result.checkpoint_data, ensure_ascii=False)}\n"
            
            report_content += "-" * 50 + "\n"
        
        # å¤±è´¥ä»»åŠ¡åˆ†æ
        if failed_tasks > 0:
            report_content += f"""
å¤±è´¥ä»»åŠ¡åˆ†æ
------------
"""
            for result in all_results:
                if not result.success:
                    report_content += f"""
â€¢ {result.task_name}: {result.error_message}
"""
        
        # å»ºè®®å’Œä¸‹ä¸€æ­¥
        report_content += f"""
å»ºè®®å’Œä¸‹ä¸€æ­¥
------------
"""
        if failed_tasks == 0:
            report_content += """
ğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼

å»ºè®®ä¸‹ä¸€æ­¥æ“ä½œï¼š
1. æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶
2. åˆ†æä¼˜åŒ–åçš„è‚½æ®µç»“æœ
3. è¿›è¡Œå®éªŒéªŒè¯
4. å‡†å¤‡ä¸‹ä¸€è½®ä¼˜åŒ–
"""
        else:
            report_content += f"""
âš ï¸ å‘ç° {failed_tasks} ä¸ªå¤±è´¥ä»»åŠ¡

å»ºè®®æ“ä½œï¼š
1. æ£€æŸ¥å¤±è´¥ä»»åŠ¡çš„é”™è¯¯ä¿¡æ¯
2. ä¿®å¤ç›¸å…³é—®é¢˜åé‡æ–°è¿è¡Œå·¥ä½œæµ
3. ä½¿ç”¨æ–­ç‚¹ç»­è·‘åŠŸèƒ½ä»å¤±è´¥æ­¥éª¤ç»§ç»­
4. è”ç³»æŠ€æœ¯æ”¯æŒè·å–å¸®åŠ©

æ–­ç‚¹ç»­è·‘å‘½ä»¤ï¼š
python workflow.py --resume-from-step [å¤±è´¥æ­¥éª¤å·]
"""
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = Path(SUMMARY_REPORT_FILE)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"æµç¨‹æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        
        # åˆ›å»ºPrefect artifact
        create_markdown_artifact(
            key="workflow-summary-report",
            markdown=report_content,
            description="è‚½æ®µè¯ç‰©å¼€å‘å·¥ä½œæµæ‰§è¡Œæ€»ç»“æŠ¥å‘Š"
        )
        
        return str(report_file)
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ€»ç»“æŠ¥å‘Šå¤±è´¥: {str(e)}")
        return ""

# ä¸»å·¥ä½œæµ
@flow(name="è‚½æ®µè¯ç‰©å¼€å‘å·¥ä½œæµ", 
      task_runner=ConcurrentTaskRunner(),
      description="å®Œæ•´çš„è‚½æ®µè¯ç‰©å¼€å‘åˆ†ææµç¨‹")
def peptide_drug_development_workflow(protein_id: str = None, resume_from_step: int = 1, 
                                      clear_cache: bool = True, species_list: List[str] = ["Human", "Mouse"]) -> Dict[str, Any]:
    """
    ä¸»å·¥ä½œæµï¼šè‚½æ®µè¯ç‰©å¼€å‘å®Œæ•´æµç¨‹
    
    Args:
        protein_id: è›‹ç™½è´¨ID
        resume_from_step: æ–­ç‚¹ç»­è·‘èµ·å§‹æ­¥éª¤ï¼ˆ1-5ï¼‰
        clear_cache: æ˜¯å¦åœ¨å¼€å§‹å‰æ¸…ç†ç¼“å­˜
        species_list: ç‰©ç§åˆ—è¡¨
    
    Returns:
        å·¥ä½œæµæ‰§è¡Œç»“æœ
    """
    logger = get_run_logger()
    workflow_manager = WorkflowManager()
    email_notifier = EmailNotifier()
    
    # åˆå§‹åŒ–å·¥ä½œæµçŠ¶æ€
    if not workflow_manager.state.start_time:
        workflow_manager.state.start_time = datetime.now()
    
    logger.info(f"å¼€å§‹è‚½æ®µè¯ç‰©å¼€å‘å·¥ä½œæµ (ä»æ­¥éª¤ {resume_from_step} å¼€å§‹)")
    
    # ä»»åŠ¡ç»“æœåˆ—è¡¨
    all_results = []
    
    # ç¼“å­˜æ¸…ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if clear_cache and resume_from_step == 1:
        logger.info("æ‰§è¡Œç¼“å­˜æ¸…ç†...")
        cache_result = task_cache_clearing(protein_id=protein_id)
        all_results.append(cache_result)
        
        if not cache_result.success:
            logger.warning(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {cache_result.error_message}")
        else:
            logger.info("ç¼“å­˜æ¸…ç†å®Œæˆ")
    
    try:
        # ä»»åŠ¡1ï¼šå‚æ•°åˆå§‹åŒ–
        if resume_from_step <= 1:
            logger.info("æ‰§è¡Œä»»åŠ¡1ï¼šå‚æ•°åˆå§‹åŒ–")
            init_result = task_parameter_initialization()
            all_results.append(init_result)
            
            if init_result.success:
                workflow_manager.mark_step_completed(1, init_result)
                logger.info("ä»»åŠ¡1å®Œæˆï¼šå‚æ•°åˆå§‹åŒ–")
            else:
                workflow_manager.mark_step_failed(1, init_result.error_message)
                raise Exception(f"ä»»åŠ¡1å¤±è´¥: {init_result.error_message}")
        else:
            logger.info("è·³è¿‡ä»»åŠ¡1ï¼šå‚æ•°åˆå§‹åŒ–ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰")
        
        # ä»»åŠ¡2ï¼šæ•°æ®æ‹‰å–
        if resume_from_step <= 2:
            logger.info("æ‰§è¡Œä»»åŠ¡2ï¼šæ•°æ®æ‹‰å–")
            fetch_result = task_data_fetching(
                all_results[0] if all_results else TaskResult(
                    task_name="å‚æ•°åˆå§‹åŒ–", success=True, start_time=datetime.now()
                ),
                protein_id=protein_id
            )
            all_results.append(fetch_result)
            
            if fetch_result.success:
                workflow_manager.mark_step_completed(2, fetch_result)
                logger.info("ä»»åŠ¡2å®Œæˆï¼šæ•°æ®æ‹‰å–")
            else:
                workflow_manager.mark_step_failed(2, fetch_result.error_message)
                raise Exception(f"ä»»åŠ¡2å¤±è´¥: {fetch_result.error_message}")
        else:
            logger.info("è·³è¿‡ä»»åŠ¡2ï¼šæ•°æ®æ‹‰å–ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰")
        
        # ä»»åŠ¡3ï¼šåˆ†æ³Œè·¯å¾„åˆ†æ
        if resume_from_step <= 3:
            logger.info("æ‰§è¡Œä»»åŠ¡3ï¼šåˆ†æ³Œè·¯å¾„åˆ†æ")
            secretion_result = task_secretion_analysis(
                all_results[1] if len(all_results) > 1 else TaskResult(
                    task_name="æ•°æ®æ‹‰å–", success=True, start_time=datetime.now()
                ),
                protein_id=protein_id
            )
            all_results.append(secretion_result)
            
            if secretion_result.success:
                workflow_manager.mark_step_completed(3, secretion_result)
                logger.info("ä»»åŠ¡3å®Œæˆï¼šåˆ†æ³Œè·¯å¾„åˆ†æ")
            else:
                workflow_manager.mark_step_failed(3, secretion_result.error_message)
                raise Exception(f"ä»»åŠ¡3å¤±è´¥: {secretion_result.error_message}")
        else:
            logger.info("è·³è¿‡ä»»åŠ¡3ï¼šåˆ†æ³Œè·¯å¾„åˆ†æï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰")
        
        # ä»»åŠ¡4ï¼šSTRINGç›¸äº’ä½œç”¨åˆ†æ
        if resume_from_step <= 4:
            logger.info("æ‰§è¡Œä»»åŠ¡4ï¼šSTRINGç›¸äº’ä½œç”¨åˆ†æ")
            string_result = task_string_interaction_analysis(
                all_results[2] if len(all_results) > 2 else TaskResult(
                    task_name="åˆ†æ³Œè·¯å¾„åˆ†æ", success=True, start_time=datetime.now()
                ),
                protein_id=protein_id
            )
            all_results.append(string_result)
            
            if string_result.success:
                workflow_manager.mark_step_completed(4, string_result)
                logger.info("ä»»åŠ¡4å®Œæˆï¼šSTRINGç›¸äº’ä½œç”¨åˆ†æ")
            else:
                workflow_manager.mark_step_failed(4, string_result.error_message)
                raise Exception(f"ä»»åŠ¡4å¤±è´¥: {string_result.error_message}")
        else:
            logger.info("è·³è¿‡ä»»åŠ¡4ï¼šSTRINGç›¸äº’ä½œç”¨åˆ†æï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰")
        
        # ä»»åŠ¡5ï¼šåˆ†å­å¯¹æ¥é¢„æµ‹
        if resume_from_step <= 5:
            logger.info("æ‰§è¡Œä»»åŠ¡5ï¼šåˆ†å­å¯¹æ¥é¢„æµ‹")
            docking_result = task_docking_prediction(
                all_results[3] if len(all_results) > 3 else TaskResult(
                    task_name="STRINGç›¸äº’ä½œç”¨åˆ†æ", success=True, start_time=datetime.now()
                ),
                protein_id=protein_id
            )
            all_results.append(docking_result)
            
            if docking_result.success:
                workflow_manager.mark_step_completed(5, docking_result)
                logger.info("ä»»åŠ¡5å®Œæˆï¼šåˆ†å­å¯¹æ¥é¢„æµ‹")
            else:
                workflow_manager.mark_step_failed(5, docking_result.error_message)
                raise Exception(f"ä»»åŠ¡5å¤±è´¥: {docking_result.error_message}")
        else:
            logger.info("è·³è¿‡ä»»åŠ¡5ï¼šåˆ†å­å¯¹æ¥é¢„æµ‹ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰")
        
        # ä»»åŠ¡6ï¼šè‚½æ®µä¼˜åŒ–
        if resume_from_step <= 6:
            logger.info("æ‰§è¡Œä»»åŠ¡6ï¼šè‚½æ®µä¼˜åŒ–")
            optimization_result = task_peptide_optimization(all_results[4] if len(all_results) > 4 else TaskResult(
                task_name="åˆ†å­å¯¹æ¥é¢„æµ‹", success=True, start_time=datetime.now()
            ))
            all_results.append(optimization_result)
            
            if optimization_result.success:
                workflow_manager.mark_step_completed(6, optimization_result)
                logger.info("ä»»åŠ¡6å®Œæˆï¼šè‚½æ®µä¼˜åŒ–")
            else:
                workflow_manager.mark_step_failed(6, optimization_result.error_message)
                raise Exception(f"ä»»åŠ¡6å¤±è´¥: {optimization_result.error_message}")
        else:
            logger.info("è·³è¿‡ä»»åŠ¡6ï¼šè‚½æ®µä¼˜åŒ–ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰")
        
        # ä»»åŠ¡7ï¼šå¯è§†åŒ–+æŠ¥å‘Šç”Ÿæˆ
        if resume_from_step <= 7:
            logger.info("æ‰§è¡Œä»»åŠ¡7ï¼šå¯è§†åŒ–+æŠ¥å‘Šç”Ÿæˆ")
            report_result = task_visualization_reporting(
                all_results[5] if len(all_results) > 5 else TaskResult(
                    task_name="è‚½æ®µä¼˜åŒ–", success=True, start_time=datetime.now()
                ),
                protein_id=protein_id
            )
            all_results.append(report_result)
            
            if report_result.success:
                workflow_manager.mark_step_completed(7, report_result)
                logger.info("ä»»åŠ¡7å®Œæˆï¼šå¯è§†åŒ–+æŠ¥å‘Šç”Ÿæˆ")
            else:
                workflow_manager.mark_step_failed(7, report_result.error_message)
                raise Exception(f"ä»»åŠ¡7å¤±è´¥: {report_result.error_message}")
        else:
            logger.info("è·³è¿‡ä»»åŠ¡7ï¼šå¯è§†åŒ–+æŠ¥å‘Šç”Ÿæˆï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        logger.info("ç”Ÿæˆæµç¨‹æ€»ç»“æŠ¥å‘Š")
        summary_report = task_generate_summary_report(all_results)
        
        # æ›´æ–°å·¥ä½œæµçŠ¶æ€
        workflow_manager.state.end_time = datetime.now()
        workflow_manager.save_state()
        
        # å‘é€æˆåŠŸé€šçŸ¥
        success_message = f"""
å·¥ä½œæµæ‰§è¡ŒæˆåŠŸå®Œæˆï¼

æ‰§è¡Œç»Ÿè®¡ï¼š
- æ€»ä»»åŠ¡æ•°: {len(all_results)}
- æˆåŠŸä»»åŠ¡æ•°: {sum(1 for r in all_results if r.success)}
- æ€»è€—æ—¶: {(workflow_manager.state.end_time - workflow_manager.state.start_time).total_seconds():.2f} ç§’

æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆ: {summary_report}
        """
        
        email_notifier.send_notification(
            subject="è‚½æ®µè¯ç‰©å¼€å‘å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ",
            message=success_message,
            attachment_path=summary_report
        )
        
        logger.info("å·¥ä½œæµæ‰§è¡ŒæˆåŠŸå®Œæˆ")
        
        return {
            "success": True,
            "total_tasks": len(all_results),
            "successful_tasks": sum(1 for r in all_results if r.success),
            "failed_tasks": sum(1 for r in all_results if not r.success),
            "total_duration": (workflow_manager.state.end_time - workflow_manager.state.start_time).total_seconds(),
            "summary_report": summary_report,
            "task_results": all_results
        }
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
        
        # å‘é€å¤±è´¥é€šçŸ¥
        error_message = f"""
å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼

é”™è¯¯ä¿¡æ¯: {str(e)}
å¤±è´¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¹¶ä¿®å¤é—®é¢˜åé‡æ–°è¿è¡Œã€‚
        """
        
        email_notifier.send_notification(
            subject="è‚½æ®µè¯ç‰©å¼€å‘å·¥ä½œæµæ‰§è¡Œå¤±è´¥",
            message=error_message,
            is_error=True
        )
        
        # æ›´æ–°å¤±è´¥çŠ¶æ€
        workflow_manager.state.end_time = datetime.now()
        workflow_manager.save_state()
        
        raise e

# æ–­ç‚¹ç»­è·‘åŠŸèƒ½
def resume_workflow_from_step(step: int, protein_id: str = None):
    """ä»æŒ‡å®šæ­¥éª¤æ¢å¤å·¥ä½œæµ"""
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ä»æ­¥éª¤ {step} æ¢å¤å·¥ä½œæµ...")
        
        # éªŒè¯æ­¥éª¤å·
        if not 1 <= step <= 5:
            raise ValueError("æ­¥éª¤å·å¿…é¡»åœ¨1-5ä¹‹é—´")
        
        # æ‰§è¡Œå·¥ä½œæµ
        result = peptide_drug_development_workflow(protein_id=protein_id, resume_from_step=step)
        
        logger.info("å·¥ä½œæµæ¢å¤æ‰§è¡Œå®Œæˆ")
        return result
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ¢å¤å¤±è´¥: {str(e)}")
        raise e

# éƒ¨ç½²é…ç½®
def create_deployment():
    """åˆ›å»ºPrefectéƒ¨ç½² - ä½¿ç”¨ç°ä»£Prefectæ–¹æ³•"""
    # ä½¿ç”¨flow.serve()æ–¹æ³•åˆ›å»ºéƒ¨ç½²
    peptide_drug_development_workflow.serve(
        name="peptide-drug-development",
        version="1.0.0",
        description="è‚½æ®µè¯ç‰©å¼€å‘å®Œæ•´å·¥ä½œæµ",
        tags=["peptide", "drug-development", "bioinformatics"],
        parameters={"resume_from_step": 1}
    )
    return "Deployment created successfully"

# å‘½ä»¤è¡Œæ¥å£
def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è‚½æ®µè¯ç‰©å¼€å‘Prefectå·¥ä½œæµ")
    parser.add_argument("--protein-id", type=str, 
                       help="è›‹ç™½è´¨ID")
    parser.add_argument("--species", type=str, 
                       help="ç‰©ç§åˆ—è¡¨ (é€—å·åˆ†éš”ï¼Œå¦‚: Human,Mouse,Rat)")
    parser.add_argument("--resume-from-step", type=int, default=1, 
                       help="æ–­ç‚¹ç»­è·‘èµ·å§‹æ­¥éª¤ (1-5)")
    parser.add_argument("--clear-cache", action="store_true", default=True,
                       help="åœ¨å¼€å§‹å‰æ¸…ç†ç¼“å­˜")
    parser.add_argument("--no-clear-cache", action="store_true",
                       help="è·³è¿‡ç¼“å­˜æ¸…ç†")
    parser.add_argument("--deploy", action="store_true", 
                       help="åˆ›å»ºPrefectéƒ¨ç½²")
    parser.add_argument("--config", type=str, default=CONFIG_FILE,
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # å¤„ç†ç¼“å­˜æ¸…ç†å‚æ•°
    clear_cache = args.clear_cache and not args.no_clear_cache
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('workflow.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    try:
        if args.deploy:
            # åˆ›å»ºéƒ¨ç½²
            logging.info("åˆ›å»ºPrefectéƒ¨ç½²...")
            result = create_deployment()
            logging.info(f"éƒ¨ç½²åˆ›å»ºæˆåŠŸ: {result}")
        else:
            # è§£æç‰©ç§åˆ—è¡¨
            species_list = None
            if args.species:
                species_list = [s.strip() for s in args.species.split(',')]
                logging.info(f"æŒ‡å®šç‰©ç§: {species_list}")
            
            # æ‰§è¡Œå·¥ä½œæµ
            if args.resume_from_step > 1:
                logging.info(f"æ–­ç‚¹ç»­è·‘æ¨¡å¼ï¼šä»æ­¥éª¤ {args.resume_from_step} å¼€å§‹")
                result = resume_workflow_from_step(args.resume_from_step, args.protein_id)
            else:
                logging.info("æ­£å¸¸æ‰§è¡Œæ¨¡å¼")
                result = peptide_drug_development_workflow(
                    protein_id=args.protein_id, 
                    clear_cache=clear_cache,
                    species_list=species_list
                )
            
            # è¾“å‡ºç»“æœ
            print(f"\nå·¥ä½œæµæ‰§è¡Œå®Œæˆ!")
            print(f"æˆåŠŸä»»åŠ¡æ•°: {result['successful_tasks']}/{result['total_tasks']}")
            print(f"æ€»è€—æ—¶: {result['total_duration']:.2f} ç§’")
            print(f"æ€»ç»“æŠ¥å‘Š: {result['summary_report']}")
            
    except Exception as e:
        logging.error(f"æ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
