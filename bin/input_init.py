#!/usr/bin/env python3
"""
è‚½æ®µè¯ç‰©å¼€å‘ - è¾“å…¥åˆå§‹åŒ–å’Œå‚æ•°é…ç½®ç³»ç»Ÿ
åŠŸèƒ½ï¼šæ¥æ”¶ç”¨æˆ·è¾“å…¥ã€éªŒè¯ç‰©ç§IDã€ç”Ÿæˆé…ç½®æ–‡ä»¶ã€è¾“å‡ºåˆ†ææµç¨‹æ¸…å•
"""

import json
import requests
import re
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import time
from datetime import datetime

class ProteinInputInitializer:
    """è›‹ç™½è´¨åˆ†æè¾“å…¥åˆå§‹åŒ–å’Œå‚æ•°é…ç½®ç³»ç»Ÿ"""
    
    def __init__(self, config_dir: str = "~/.peptide_env"):
        self.config_dir = Path(config_dir).expanduser()
        self.config_dir.mkdir(parents=True, exist_ok=True)
        
        # NCBI APIé…ç½®
        self.ncbi_base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.max_retries = 3
        self.request_delay = 0.5  # APIè¯·æ±‚é—´éš”
        
        # æ•°æ®åº“è·¯å¾„é…ç½®
        self.database_paths = {
            "uniprot": "./data/uniprot/",
            "pdb": "./data/pdb/",
            "string": "./data/string/",
            "kegg": "./data/kegg/",
            "pfam": "./data/pfam/",
            "reactome": "./data/reactome/"
        }
        
        # å®éªŒè®¾å¤‡APIæ¥å£ï¼ˆé¢„ç•™ï¼‰
        self.equipment_apis = {
            "peptide_synthesizer": "http://localhost:8080/api/synthesizer",
            "mass_spectrometer": "http://localhost:8081/api/ms",
            "hplc": "http://localhost:8082/api/hplc",
            "cd_spectrometer": "http://localhost:8083/api/cd"
        }
        
        # åˆ†æç›®æ ‡é€‰é¡¹
        self.analysis_options = {
            "1": "åˆ†æ³Œè·¯å¾„è§£æ",
            "2": "å—ä½“å‘ç°", 
            "3": "è‚½æ®µä¼˜åŒ–",
            "4": "æ¯’æ€§é¢„æµ‹",
            "5": "ç”Ÿç‰©æ´»æ€§è¯„ä¼°",
            "6": "ç¨³å®šæ€§åˆ†æ"
        }

    def get_user_input(self) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·è¾“å…¥"""
        print("ğŸ§¬ è‚½æ®µè¯ç‰©å¼€å‘ - è¾“å…¥åˆå§‹åŒ–ç³»ç»Ÿ")
        print("=" * 60)
        
        input_data = {}
        
        # 1. è›‹ç™½åç§°è¾“å…¥
        while True:
            protein_name = input("\nğŸ”¬ è¯·è¾“å…¥è›‹ç™½è´¨åç§° (å¦‚: THBS4, TNF-Î±, IL-6): ").strip()
            if self.validate_protein_name(protein_name):
                input_data['protein_name'] = protein_name
                break
            else:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„è›‹ç™½è´¨åç§°ï¼ˆå­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦ã€ä¸‹åˆ’çº¿ï¼‰")
        
        # 2. ç‰©ç§IDè¾“å…¥
        print(f"\nğŸŒ è¯·è¾“å…¥ç›®æ ‡ç‰©ç§ID (æ ¼å¼: ç‰©ç§å+è›‹ç™½ID, å¤šç‰©ç§ç”¨é€—å·åˆ†éš”)")
        print("ç¤ºä¾‹: äººNP_003253.1,å°é¼ NP_035712.1,ç»†èŒYP_123456.1")
        
        species_input = input("ç‰©ç§åˆ—è¡¨: ").strip()
        validated_species = self.get_and_validate_species(species_input)
        input_data['species_data'] = validated_species
        
        # 3. åˆ†æç›®æ ‡é€‰æ‹©
        print(f"\nğŸ¯ è¯·é€‰æ‹©åˆ†æç›®æ ‡ (å¤šé€‰ï¼Œè¾“å…¥æ•°å­—åºå·):")
        for key, value in self.analysis_options.items():
            print(f"  {key}. {value}")
        
        selected_analyses = self.get_analysis_selections()
        input_data['analysis_targets'] = selected_analyses
        
        # 4. é¢å¤–é…ç½®
        input_data.update(self.get_additional_config())
        
        return input_data

    def validate_protein_name(self, name: str) -> bool:
        """éªŒè¯è›‹ç™½è´¨åç§°æ ¼å¼"""
        if not name:
            return False
        
        # å…è®¸å­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦ã€ä¸‹åˆ’çº¿ã€å¸Œè…Šå­—æ¯ç­‰
        pattern = r'^[a-zA-Z0-9Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰Î‘Î’Î“Î”Î•Î–Î—Î˜Î™ÎšÎ›ÎœÎÎÎŸÎ Î¡Î£Î¤Î¥Î¦Î§Î¨Î©\-_\.\s]+$'
        return bool(re.match(pattern, name))

    def get_and_validate_species(self, species_input: str) -> List[Dict[str, str]]:
        """è§£æå’ŒéªŒè¯ç‰©ç§ID"""
        species_list = [s.strip() for s in species_input.split(',') if s.strip()]
        validated_species = []
        
        print(f"\nğŸ” æ­£åœ¨éªŒè¯ {len(species_list)} ä¸ªç‰©ç§ID...")
        
        for i, species_entry in enumerate(species_list, 1):
            print(f"  éªŒè¯ç¬¬ {i}/{len(species_list)} ä¸ª: {species_entry}")
            
            parsed_species = self.parse_species_entry(species_entry)
            if parsed_species:
                validation_result = self.validate_ncbi_id(parsed_species['protein_id'])
                
                if validation_result['valid']:
                    parsed_species['validation'] = validation_result
                    validated_species.append(parsed_species)
                    print(f"    âœ… {validation_result['title']} - {validation_result['organism']}")
                else:
                    print(f"    âŒ IDæ— æ•ˆ: {validation_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
                    # è‡ªåŠ¨ä¿®æ­£å»ºè®®
                    corrections = self.suggest_protein_corrections(parsed_species['species'], parsed_species['protein_id'])
                    if corrections:
                        print(f"    ğŸ’¡ å»ºè®®ä¿®æ­£ä¸º:")
                        for correction in corrections[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                            print(f"      - {correction}")
            else:
                print(f"    âŒ æ ¼å¼é”™è¯¯: è¯·ä½¿ç”¨ 'ç‰©ç§å+è›‹ç™½ID' æ ¼å¼")
            
            # APIè¯·æ±‚é—´éš”
            time.sleep(self.request_delay)
        
        return validated_species

    def parse_species_entry(self, entry: str) -> Optional[Dict[str, str]]:
        """è§£æç‰©ç§æ¡ç›®"""
        # åŒ¹é…æ ¼å¼: ç‰©ç§å + è›‹ç™½ID
        patterns = [
            r'(.+?)([NPGQY]P_\d+\.\d+)',  # NP_123456.1, YP_123456.1ç­‰
            r'(.+?)([A-Z]{1,4}\d{5,8}\.?\d*)',  # å…¶ä»–æ ¼å¼ID
        ]
        
        for pattern in patterns:
            match = re.search(pattern, entry)
            if match:
                species_name = match.group(1).strip()
                protein_id = match.group(2).strip()
                return {
                    'species': species_name,
                    'protein_id': protein_id,
                    'original_entry': entry
                }
        
        return None

    def validate_ncbi_id(self, protein_id: str) -> Dict[str, Any]:
        """éªŒè¯NCBIDè›‹ç™½ID"""
        try:
            # æœç´¢proteinæ•°æ®åº“
            search_url = f"{self.ncbi_base_url}esearch.fcgi"
            params = {
                'db': 'protein',
                'term': protein_id,
                'retmode': 'json',
                'retmax': 1
            }
            
            response = None
            for attempt in range(self.max_retries):
                try:
                    response = requests.get(search_url, params=params, timeout=10)
                    if response.status_code == 200:
                        break
                    else:
                        time.sleep(self.request_delay)
                except Exception as e:
                    if attempt == self.max_retries - 1:
                        return {'valid': False, 'error': f'ç½‘ç»œé”™è¯¯: {str(e)}'}
                    time.sleep(self.request_delay)
            
            if not response or response.status_code != 200:
                return {'valid': False, 'error': 'APIè¯·æ±‚å¤±è´¥'}
            
            data = response.json()
            
            if 'esearchresult' not in data:
                return {'valid': False, 'error': 'APIå“åº”æ ¼å¼é”™è¯¯'}
            
            id_list = data['esearchresult'].get('idlist', [])
            
            if not id_list:
                return {'valid': False, 'error': 'æœªæ‰¾åˆ°å¯¹åº”çš„è›‹ç™½è´¨è®°å½•'}
            
            # è·å–è¯¦ç»†ä¿¡æ¯
            detail_url = f"{self.ncbi_base_url}efetch.fcgi"
            detail_params = {
                'db': 'protein',
                'id': id_list[0],
                'retmode': 'xml',
                'rettype': 'fasta'
            }
            
            detail_response = None
            for attempt in range(self.max_retries):
                try:
                    detail_response = requests.get(detail_url, params=detail_params, timeout=15)
                    if detail_response.status_code == 200:
                        break
                    else:
                        time.sleep(self.request_delay)
                except Exception as e:
                    if attempt == self.max_retries - 1:
                        return {'valid': False, 'error': f'è·å–è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}'}
                    time.sleep(self.request_delay)
            
            if detail_response and detail_response.status_code == 200:
                fasta_data = detail_response.text
                
                # ç®€å•è§£æFASTAæ ‡é¢˜
                lines = fasta_data.split('\n')
                for line in lines:
                    if line.startswith('>'):
                        title = line[1:].strip()
                        return {
                            'valid': True,
                            'ncbi_id': id_list[0],
                            'title': title,
                            'organism': self.extract_organism_from_title(title),
                            'length': len(''.join(lines[1:]).replace('\n', ''))
                        }
            
            return {'valid': False, 'error': 'è§£æè¯¦ç»†ä¿¡æ¯å¤±è´¥'}
            
        except Exception as e:
            return {'valid': False, 'error': f'éªŒè¯è¿‡ç¨‹é”™è¯¯: {str(e)}'}

    def extract_organism_from_title(self, title: str) -> str:
        """ä»FASTAæ ‡é¢˜æå–ç‰©ç§å"""
        # ç®€å•æå–ç¬¬ä¸€ä¸ªæ–¹æ‹¬å·æˆ–æ‹¬å·å†…çš„å†…å®¹
        patterns = [
            r'\[([^\]]+)\]',  # [ç‰©ç§å]
            r'\(([^\)]+)\)',  # (ç‰©ç§å)
            r'(\w+)\s*\w*\s*gene',  # ç‰©ç§å gene
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title)
            if match:
                org = match.group(1).strip()
                if len(org) > 3:  # ç‰©ç§åé€šå¸¸è¾ƒé•¿
                    return org
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›å‰å‡ ä¸ªè¯
        words = title.split()
        if len(words) >= 2:
            return f"{words[0]} {words[1]}"
        
        return "æœªçŸ¥ç‰©ç§"

    def suggest_protein_corrections(self, species: str, protein_id: str) -> List[str]:
        """ä¸ºæ— æ•ˆIDæä¾›ä¿®æ­£å»ºè®®"""
        try:
            # åŸºäºç‰©ç§åæœç´¢ç›¸å…³è›‹ç™½è´¨
            search_term = f"{species}[organism] AND {protein_id.split('.')[0]}[accession]"
            search_url = f"{self.ncbi_base_url}esearch.fcgi"
            params = {
                'db': 'protein',
                'term': search_term,
                'retmode': 'json',
                'retmax': 5
            }
            
            response = requests.get(search_url, params=params, timeout=10)
            if response.status_code != 200:
                return []
            
            data = response.json()
            if 'esearchresult' not in data:
                return []
            
            suggestions = []
            id_list = data['esearchresult'].get('idlist', [])[:3]
            
            for ncbi_id in id_list:
                detail_url = f"{self.ncbi_base_url}efetch.fcgi"
                detail_params = {
                    'db': 'protein',
                    'id': ncbi_id,
                    'retmode': 'xml',
                    'rettype': 'fasta'
                }
                
                detail_response = requests.get(detail_url, params=detail_params, timeout=10)
                if detail_response.status_code == 200:
                    fasta_data = detail_response.text
                    title_line = None
                    accession_line = None
                    
                    for line in fasta_data.split('\n'):
                        if line.startswith('>'):
                            title_line = line[1:].strip()
                        elif 'VERSION' in line and 'ACCESSION' in line:
                            accession_match = re.search(r'ACCESSION\s+(\S+)', line)
                            if accession_match:
                                accession_line = accession_match.group(1)
                    
                    if title_line and accession_line:
                        org_name = self.extract_organism_from_title(title_line)
                        suggestions.append(f"{org_name} {accession_line}")
                break
            
            return suggestions
            
        except Exception:
            return []

    def get_analysis_selections(self) -> List[str]:
        """è·å–åˆ†æç›®æ ‡é€‰æ‹©"""
        while True:
            selections = input("\nè¯·é€‰æ‹©åˆ†æç›®æ ‡ (å¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 1,3,5): ").strip()
            
            if not selections:
                print("âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªåˆ†æç›®æ ‡")
                continue
            
            try:
                selected_numbers = [s.strip() for s in selections.split(',')]
                validated_selections = []
                
                for num in selected_numbers:
                    if num in self.analysis_options:
                                validated_selections.append(self.analysis_options[num])
                    else:
                        print(f"âŒ æ— æ•ˆé€‰æ‹©: {num}")
                        break
                else:
                    # æ‰€æœ‰é€‰æ‹©éƒ½æœ‰æ•ˆ
                    return validated_selections
                    
            except Exception:
                print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")

    def get_additional_config(self) -> Dict[str, Any]:
        """è·å–é¢å¤–é…ç½®"""
        additional = {}
        
        print(f"\nâš™ï¸  é¢å¤–é…ç½®é€‰é¡¹:")
        
        # ä¼˜å…ˆçº§è®¾ç½®
        priority = input("è®¾ç½®åˆ†æä¼˜å…ˆçº§ (high/medium/lowï¼Œé»˜è®¤: medium): ").strip().lower()
        if priority in ['high', 'medium', 'low']:
            additional['priority'] = priority
        else:
            additional['priority'] = 'medium'
        
        # è¾“å‡ºè·¯å¾„
        output_path = input("æŒ‡å®šè¾“å‡ºè·¯å¾„ (å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
        if output_path:
            additional['custom_output_path'] = Path(output_path).resolve()
        
        # é‚®ä»¶é€šçŸ¥
        email = input("é‚®ç®±é€šçŸ¥åœ°å€ (å¯é€‰): ").strip()
        if email and '@' in email:
            additional['notification_email'] = email
        
        return additional

    def generate_config_json(self, input_data: Dict[str, Any]) -> Path:
        """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
        config = {
            "project_info": {
                "protein_name": input_data['protein_name'],
                "created_time": datetime.now().isoformat(),
                "version": "1.0"
            },
            "species_data": input_data['species_data'],
            "analysis_targets": input_data['analysis_targets'],
            "priority": input_data.get('priority', 'medium'),
            "database_paths": self.database_paths,
            "equipment_apis": self.equipment_apis,
            "output_settings": {
                "default_path": str(Path.home() / "peptide_analysis_results"),
                "custom_path": input_data.get('custom_output_path'),
                "formats": ["json", "pdf", "excel"],
                "images_format": "png"
            },
            "notification": {
                "email": input_data.get('notification_email'),
                "webhook_url": None
            },
            "advanced_settings": {
                "api_timeout": 30,
                "max_retries": 3,
                "parallel_processing": True,
                "cache_results": True
            }
        }
        
        config_file = self.config_dir / f"{input_data['protein_name'].lower()}_config.json"
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {config_file}")
        return config_file

    def generate_analysis_workflow(self, input_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆåˆ†ææµç¨‹æ¸…å•"""
        workflow = []
        
        protein_name = input_data['protein_name']
        analysis_targets = input_data['analysis_targets']
        species_count = len(input_data['species_data'])
        
        # é€šç”¨é¢„å¤„ç†æ­¥éª¤
        workflow.append({
            "step": 1,
            "task": "æ•°æ®æ”¶é›†å’ŒéªŒè¯",
            "description": f"æ”¶é›† {protein_name} çš„å¤šç‰©ç§åºåˆ—æ•°æ®",
            "dependencies": [],
            "estimated_time": "2-5åˆ†é’Ÿ",
            "status": "å¾…å¼€å§‹"
        })
        
        workflow.append({
            "step": 2,
            "task": "åºåˆ—æ¯”å¯¹å’Œä¿å®ˆæ€§åˆ†æ",
            "description": f"åˆ†æ {protein_name} åœ¨{species_count}ä¸ªç‰©ç§é—´çš„ä¿å®ˆæ€§",
            "dependencies": [1],
            "estimated_time": "5-10åˆ†é’Ÿ",
            "status": "å¾…å¼€å§‹"
        })
        
        # æ ¹æ®åˆ†æç›®æ ‡æ·»åŠ æ­¥éª¤
        step_counter = 3
        
        if "åˆ†æ³Œè·¯å¾„è§£æ" in analysis_targets:
            workflow.append({
                "step": step_counter,
                "task": "åˆ†æ³Œè·¯å¾„é¢„æµ‹",
                "description": f"ä½¿ç”¨SignalP-6åˆ†æ {protein_name} çš„ä¿¡å·è‚½å’Œåˆ†æ³Œç‰¹æ€§",
                "dependencies": [1, 2],
                "estimated_time": "3-5åˆ†é’Ÿ",
                "status": "å¾…å¼€å§‹",
                "tools": ["SignalP-6", "PSORTb", "SecretP"]
            })
            step_counter += 1
        
        if "å—ä½“å‘ç°" in analysis_targets:
            workflow.append({
                "step": step_counter,
                "task": "å—ä½“ç›¸äº’ä½œç”¨é¢„æµ‹",
                "description": f"é¢„æµ‹ {protein_name} å¯èƒ½ç»“åˆçš„å—ä½“å’Œç›¸äº’ä½œç”¨ä½ç‚¹",
                "dependencies": [1, 2],
                "estimated_time": "10-15åˆ†é’Ÿ",
                "status": "å¾…å¼€å§‹",
                "tools": ["STRING", "HINTdb", "Interactome3D"]
            })
            workflow.append({
                "step": step_counter + 1,
                "task": "å—ä½“-é…ä½“ç»“åˆæ¨¡å‹",
                "description": f"æ„å»º {protein_name} ä¸å—ä½“è›‹ç™½çš„ç»“åˆæ¨¡å‹",
                "dependencies": [step_counter],
                "estimated_time": "15-30åˆ†é’Ÿ",
                "status": "å¾…å¼€å§‹",
                "tools": ["AutoDock Vina", "PyMOL"]
            })
            step_counter += 2
        
        if "è‚½æ®µä¼˜åŒ–" in analysis_targets:
            workflow.append({
                "step": step_counter,
                "task": "è‚½æ®µè®¾è®¡ä¼˜åŒ–",
                "description": f"åŸºäºä¿å®ˆæ€§åˆ†æè®¾è®¡ä¼˜åŒ–çš„ {protein_name} è‚½æ®µ",
                "dependencies": [1, 2],
                "estimated_time": "20-40åˆ†é’Ÿ",
                "status": "å¾…å¼€å§‹",
                "tools": ["ProGen2", "AlphaFold2", "Rosetta"]
            })
            workflow.append({
                "step": step_counter + 1,
                "task": "ç”Ÿç‰©æ´»æ€§è¯„åˆ†",
                "description": f"è¯„ä¼°ä¼˜åŒ–åè‚½æ®µçš„ç”Ÿç‰©æ´»æ€§å’ŒåŠŸèƒ½è¯„åˆ†",
                "dependencies": [step_counter],
                "estimated_time": "10-20åˆ†é’Ÿ",
                "status": "å¾…å¼€å§‹",
                "tools": ["Bio-Activity-Predictor", "QSAR"]
            })
            step_counter += 2
        
        if "æ¯’æ€§é¢„æµ‹" in analysis_targets:
            workflow.append({
                "step": step_counter,
                "task": "æ¯’æ€§è¯„ä¼°",
                "description": f"é¢„æµ‹ {protein_name} è‚½æ®µçš„æ½œåœ¨æ¯’æ€§å’Œå‰¯ä½œç”¨",
                "dependencies": [2],
                "estimated_time": "5-10åˆ†é’Ÿ",
                "status": "å¾…å¼€å§‹",
                "tools": ["ToxPred", "ADMET-SAR"]
            })
            step_counter += 1
        
        if "ç”Ÿç‰©æ´»æ€§è¯„ä¼°" in analysis_targets:
            workflow.append({
                "step": step_counter,
                "task": "ç”Ÿç‰©æ´»æ€§é¢„æµ‹",
                "description": f"é¢„æµ‹ {protein_name} è‚½æ®µçš„ç”Ÿç‰©æ´»æ€§å’Œè¯ç†ä½œç”¨",
                "dependencies": [2],
                "estimated_time": "10-15åˆ†é’Ÿ",
                "status": "å¾…å¼€å§‹",
                "tools": ["ChEMBL", "PADIF", "Activity-Predictor"]
            })
            step_counter += 1
        
        if "ç¨³å®šæ€§åˆ†æ" in analysis_targets:
            workflow.append({
                "step": step_counter,
                "task": "ç¨³å®šæ€§é¢„æµ‹",
                "description": f"åˆ†æ {protein_name} è‚½æ®µçš„ç»“æ„ç¨³å®šæ€§å’Œé™è§£ç‰¹æ€§",
                "dependencies": [2],
                "estimated_time": "8-12åˆ†é’Ÿ",
                "status": "å¾…å¼€å§‹",
                "tools": ["FoldX", "PELE", "GROMACS"]
            })
            step_counter += 1
        
        # é€šç”¨åå¤„ç†æ­¥éª¤
        workflow.append({
            "step": step_counter,
            "task": "ç»“æœæ•´åˆä¸æŠ¥å‘Šç”Ÿæˆ",
            "description": f"æ•´åˆæ‰€æœ‰åˆ†æç»“æœï¼Œç”Ÿæˆ {protein_name} çš„ç»¼åˆåˆ†ææŠ¥å‘Š",
            "dependencies": list(range(step_counter)),
            "estimated_time": "5-10åˆ†é’Ÿ",
            "status": "å¾…å¼€å§‹",
            "tools": ["ReportLab", "Matplotlib", "Streamlit"]
        })
        
        return workflow

    def display_workflow_summary(self, workflow: List[Dict[str, Any]]):
        """æ˜¾ç¤ºæµç¨‹å¯åŠ¨æ¸…å•"""
        print(f"\nğŸ“‹ åˆ†ææµç¨‹å¯åŠ¨æ¸…å•")
        print("=" * 80)
        
        total_steps = len(workflow)
        estimated_total_time = 0
        
        for step_info in workflow:
            step = step_info['step']
            task = step_info['task']
            desc = step_info['description']
            deps = step_info['dependencies']
            time_est = step_info['estimated_time']
            tools = step_info.get('tools', [])
            
            print(f"\n{step:2d}. {task}")
            print(f"    ğŸ“ {desc}")
            
            if deps:
                dep_str = ', '.join([f"æ­¥éª¤{d}" for d in deps])
                print(f"    ğŸ“Œ ä¾èµ–: {dep_str}")
            
            print(f"    â±ï¸  é¢„è®¡æ—¶é—´: {time_est}")
            
            if tools:
                tools_str = ', '.join(tools)
                print(f"    ğŸ› ï¸  ä½¿ç”¨å·¥å…·: {tools_str}")
            
            print(f"    ğŸ“Š çŠ¶æ€: {step_info['status']}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æµç¨‹ç»Ÿè®¡:")
        print(f"    æ€»æ­¥éª¤æ•°: {total_steps}")
        print(f"    åˆ†æç›®æ ‡: {', '.join(self.analysis_targets)}")
        
        # é¢„ä¼°æ€»ä½“æ—¶é—´
        time_ranges = []
        for step in workflow:
            time_str = step['estimated_time']
            if '-' in time_str and 'åˆ†é’Ÿ' in time_str:
                min_time, max_time = map(int, time_str.replace('åˆ†é’Ÿ', '').split('-'))
                time_ranges.append((min_time, max_time))
        
        if time_ranges:
            total_min = sum(r[0] for r in time_ranges)
            total_max = sum(r[1] for r in time_ranges)
            print(f"    é¢„è®¡æ€»æ—¶é—´: {total_min}-{total_max}åˆ†é’Ÿ")
        
        print(f"\nğŸš€ å‡†å¤‡å¯åŠ¨åˆ†ææµç¨‹!")

    def run(self):
        """è¿è¡Œè¾“å…¥åˆå§‹åŒ–ç³»ç»Ÿ"""
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            input_data = self.get_user_input()
            
            # ç”Ÿæˆé…ç½®æ–‡ä»¶
            config_file = self.generate_config_json(input_data)
            
            # ç”Ÿæˆåˆ†ææµç¨‹
            workflow = self.generate_analysis_workflow(input_data)
            
            # æ˜¾ç¤ºæµç¨‹æ¸…å•
            self.display_workflow_summary(workflow)
            
            # ä¿å­˜æµç¨‹é…ç½®
            workflow_file = self.config_dir / f"{input_data['protein_name'].lower()}_workflow.json"
            with open(workflow_file, 'w', encoding='utf-8') as f:
                json.dump(workflow, f, ensure_ascii=False, indent=2)
            
            print(f"\nâœ… æµç¨‹é…ç½®å·²ä¿å­˜: {workflow_file}")
            print(f"\nğŸ¯ ä¸‹ä¸€æ­¥: è¿è¡Œåˆ†æè„šæœ¬å¼€å§‹å¤„ç†!")
            print(f"   é…ç½®æ–‡ä»¶: {config_file}")
            print(f"   æµç¨‹æ–‡ä»¶: {workflow_file}")
            
            return {
                'config_file': config_file,
                'workflow_file': workflow_file,
                'input_data': input_data,
                'workflow': workflow
            }
            
        except KeyboardInterrupt:
            print(f"\n\nâ¹ï¸  ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return None
        except Exception as e:
            print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {str(e)}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    initializer = ProteinInputInitializer()
    result = initializer.run()
    
    if result:
        print(f"\nğŸ‰ è¾“å…¥åˆå§‹åŒ–å®Œæˆ!")
        print(f"ğŸ“ é…ç½®æ–‡ä»¶: {result['config_file']}")
        print(f"ğŸ“‹ æµç¨‹æ–‡ä»¶: {result['workflow_file']}")
        
        # è¿”å›ç»“æœä»¥ä¾¿åç»­è„šæœ¬ä½¿ç”¨
        return result
    else:
        print(f"\nâŒ åˆå§‹åŒ–å¤±è´¥")
        return None

if __name__ == "__main__":
    main()
